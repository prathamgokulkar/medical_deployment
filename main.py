{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27351d19-4646-4c59-8f4e-bef0fb253180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.8690 - loss: 0.4462 - val_accuracy: 0.9687 - val_loss: 0.1112\n",
      "Epoch 2/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9660 - loss: 0.1105 - val_accuracy: 0.9733 - val_loss: 0.0881\n",
      "Epoch 3/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9768 - loss: 0.0739 - val_accuracy: 0.9775 - val_loss: 0.0799\n",
      "Epoch 4/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.9824 - loss: 0.0534 - val_accuracy: 0.9750 - val_loss: 0.0828\n",
      "Epoch 5/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.9853 - loss: 0.0445 - val_accuracy: 0.9798 - val_loss: 0.0816\n",
      "Epoch 6/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9896 - loss: 0.0315 - val_accuracy: 0.9763 - val_loss: 0.0774\n",
      "Epoch 7/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.9907 - loss: 0.0270 - val_accuracy: 0.9753 - val_loss: 0.0975\n",
      "Epoch 8/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9926 - loss: 0.0250 - val_accuracy: 0.9807 - val_loss: 0.0761\n",
      "Epoch 9/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.9945 - loss: 0.0168 - val_accuracy: 0.9765 - val_loss: 0.0991\n",
      "Epoch 10/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 9ms/step - accuracy: 0.9950 - loss: 0.0152 - val_accuracy: 0.9765 - val_loss: 0.0991\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9764 - loss: 0.0952\n",
      "Test Accuracy: 0.9789\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# === 1. Load and Preprocess Data ===\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize image pixel values (0–255) to (0–1)\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# One-hot encode labels (0-9 => [0,0,0,1,0,...])\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# === 2. Build the Feedforward Neural Network ===\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),   # Converts 28x28 image to 784 vector\n",
    "    Dense(128, activation='relu'),  # Hidden layer\n",
    "    Dense(64, activation='relu'),   # Hidden layer\n",
    "    Dense(10, activation='softmax') # Output layer (10 classes)\n",
    "])\n",
    "\n",
    "# === 3. Compile the Model ===\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# === 4. Train the Model ===\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# === 5. Evaluate the Model ===\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6271e20-d363-49b7-8305-35ceab59cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.8711 - loss: 0.4417 - val_accuracy: 0.9698 - val_loss: 0.0998\n",
      "Epoch 2/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.9663 - loss: 0.1098 - val_accuracy: 0.9743 - val_loss: 0.0891\n",
      "Epoch 3/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.9770 - loss: 0.0734 - val_accuracy: 0.9730 - val_loss: 0.0900\n",
      "Epoch 4/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.9829 - loss: 0.0560 - val_accuracy: 0.9790 - val_loss: 0.0735\n",
      "Epoch 5/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9860 - loss: 0.0430 - val_accuracy: 0.9780 - val_loss: 0.0775\n",
      "Epoch 6/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9888 - loss: 0.0336 - val_accuracy: 0.9802 - val_loss: 0.0711\n",
      "Epoch 7/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9915 - loss: 0.0256 - val_accuracy: 0.9773 - val_loss: 0.0824\n",
      "Epoch 8/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 9ms/step - accuracy: 0.9925 - loss: 0.0229 - val_accuracy: 0.9817 - val_loss: 0.0766\n",
      "Epoch 9/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.9930 - loss: 0.0197 - val_accuracy: 0.9788 - val_loss: 0.0864\n",
      "Epoch 10/10\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.9953 - loss: 0.0147 - val_accuracy: 0.9790 - val_loss: 0.0860\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9745 - loss: 0.1024\n",
      "Test Accuracy: 0.9768\n"
     ]
    }
   ],
   "source": [
    " import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# === 1. Load MNIST dataset ===\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# === 2. Normalize image pixel values to range [0, 1] ===\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# === 3. One-hot encode labels (for 10 classes: 0–9) ===\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# === 4. Build Deep Neural Network ===\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),       # Flatten image to 784 input neurons\n",
    "    Dense(128, activation='relu'),       # Hidden layer\n",
    "    Dense(64, activation='relu'),        # Another hidden layer\n",
    "    Dense(10, activation='softmax')      # Output layer (10 classes)\n",
    "])\n",
    "\n",
    "# === 5. Compile the model ===\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# === 6. Train the model ===\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# === 7. Evaluate the model ===\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f8f41c-7423-47fb-9a25-34fcd2182dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 15ms/step - accuracy: 0.6835 - loss: 0.5800 - val_accuracy: 0.8692 - val_loss: 0.3204\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.8892 - loss: 0.2717 - val_accuracy: 0.8764 - val_loss: 0.2967\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.9240 - loss: 0.2012 - val_accuracy: 0.8550 - val_loss: 0.3531\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9384 - loss: 0.1657 - val_accuracy: 0.8726 - val_loss: 0.3167\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - accuracy: 0.9495 - loss: 0.1424 - val_accuracy: 0.8708 - val_loss: 0.3475\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.9499 - loss: 0.1404 - val_accuracy: 0.8734 - val_loss: 0.3539\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9658 - loss: 0.1029 - val_accuracy: 0.8638 - val_loss: 0.4086\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 15ms/step - accuracy: 0.9643 - loss: 0.1045 - val_accuracy: 0.8658 - val_loss: 0.4197\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 14ms/step - accuracy: 0.9722 - loss: 0.0837 - val_accuracy: 0.8648 - val_loss: 0.4667\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9745 - loss: 0.0784 - val_accuracy: 0.8680 - val_loss: 0.4862\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8534 - loss: 0.5371\n",
      "\n",
      "Test Accuracy: 0.8502\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# === 1. Load IMDB Dataset (preprocessed & encoded) ===\n",
    "vocab_size = 10000  # Use top 10,000 words\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# === 2. Pad Sequences to same length ===\n",
    "maxlen = 200  # Max review length\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# === 3. Build the DNN Model ===\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=32, input_length=maxlen),\n",
    "    GlobalAveragePooling1D(),  # Averages word embeddings\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Binary output\n",
    "])\n",
    "\n",
    "# === 4. Compile the Model ===\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# === 5. Train the Model ===\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# === 6. Evaluate the Model ===\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99fcb763-27aa-4368-b16f-921377c280f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LOQ\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 27ms/step - accuracy: 0.8872 - loss: 0.3596 - val_accuracy: 0.9858 - val_loss: 0.0486\n",
      "Epoch 2/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 28ms/step - accuracy: 0.9829 - loss: 0.0541 - val_accuracy: 0.9885 - val_loss: 0.0416\n",
      "Epoch 3/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 31ms/step - accuracy: 0.9891 - loss: 0.0353 - val_accuracy: 0.9892 - val_loss: 0.0385\n",
      "Epoch 4/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 32ms/step - accuracy: 0.9917 - loss: 0.0254 - val_accuracy: 0.9912 - val_loss: 0.0347\n",
      "Epoch 5/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 30ms/step - accuracy: 0.9938 - loss: 0.0189 - val_accuracy: 0.9905 - val_loss: 0.0341\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.9876 - loss: 0.0399\n",
      "\n",
      "Test Accuracy: 0.9906\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# === 1. Load and Preprocess Data ===\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape to (samples, height, width, channels)\n",
    "X_train = X_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "X_test = X_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# === 2. Build CNN Model ===\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # Output: 10 classes\n",
    "])\n",
    "\n",
    "# === 3. Compile the Model ===\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# === 4. Train the Model ===\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# === 5. Evaluate the Model ===\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105fdfa-b541-4e84-be97-c7e4f75f94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# === 1. Load your dataset ===\n",
    "# Replace with your actual CSV path\n",
    "df = pd.read_csv(r\"D:\\Data Science\\Practice Datasets\\IMDB Dataset.csv\\IMDB Dataset.csv\")  # Must have 'text' and 'sentiment' columns\n",
    "\n",
    "# === 2. Encode labels (0: Negative, 1: Neutral, 2: Positive) ===\n",
    "label_encoder = LabelEncoder()\n",
    "df['sentiment'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# === 3. Preprocess text ===\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "padded = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# === 4. Train/Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# === 5. Build RNN Model using LSTM ===\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64, input_length=100),\n",
    "    LSTM(64),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 classes\n",
    "])\n",
    "\n",
    "# === 6. Compile ===\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# === 7. Train ===\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# === 8. Evaluate ===\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1c593-51a2-4e86-9761-8b79a985f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense\n",
    "\n",
    "# === 1. Load and prepare dataset ===\n",
    "df = pd.read_csv('D:\\Data Science\\Practice Datasets\\weatherHistory.csv\\weatherHistory.csv')  # Adjust path if needed\n",
    "df['Formatted Date'] = pd.to_datetime(df['Formatted Date'])\n",
    "df.set_index('Formatted Date', inplace=True)\n",
    "\n",
    "# Use only temperature column\n",
    "data = df[['Temperature (C)']]\n",
    "\n",
    "# === 2. Normalize data ===\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# === 3. Create sequences ===\n",
    "def create_sequences(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:i+window_size])\n",
    "        y.append(data[i+window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "window_size = 30\n",
    "X, y = create_sequences(scaled_data, window_size)\n",
    "\n",
    "# === 4. Split into train/test ===\n",
    "split = int(len(X) * 0.8)\n",
    "X_train, y_train = X[:split], y[:split]\n",
    "X_test, y_test = X[split:], y[split:]\n",
    "\n",
    "# === 5. Build LSTM Model ===\n",
    "def build_lstm():\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(window_size, 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# === 6. Build GRU Model ===\n",
    "def build_gru():\n",
    "    model = Sequential([\n",
    "        GRU(64, input_shape=(window_size, 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# === 7. Train and evaluate both models ===\n",
    "print(\"Training LSTM...\")\n",
    "lstm_model = build_lstm()\n",
    "lstm_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "print(\"\\nTraining GRU...\")\n",
    "gru_model = build_gru()\n",
    "gru_model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# === 8. Forecasting ===\n",
    "lstm_preds = lstm_model.predict(X_test)\n",
    "gru_preds = gru_model.predict(X_test)\n",
    "\n",
    "# Inverse scale\n",
    "lstm_preds = scaler.inverse_transform(lstm_preds)\n",
    "gru_preds = scaler.inverse_transform(gru_preds)\n",
    "y_true = scaler.inverse_transform(y_test)\n",
    "\n",
    "# === 9. Plot Results ===\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(y_true, label='True')\n",
    "plt.plot(lstm_preds, label='LSTM Forecast')\n",
    "plt.plot(gru_preds, label='GRU Forecast')\n",
    "plt.title(\"Temperature Forecasting\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
